{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkCKFVHa8UG9fLFeSABdjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viti990/my_own_llama/blob/main/llamav27b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aviator LLaMA\n",
        "\n",
        "This notebook aims at reconstructing LLAMA 2 7B architecture and use the model weights trained from meta to learn how it is done!\n",
        "\n",
        "Also the aim is to adapt the model with LoRA and QLoRA for PEFT!\n",
        "\n",
        "The fine tuning will be done with aviation regulations from 14 CFR...\n"
      ],
      "metadata": {
        "id": "H57i8tK00XR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Downloading the weights\n",
        "\n",
        "Downloading the weights from meta for 7B-chat (i want the model to be able to respond to questions from the requirements)\n",
        "for this one must go to the meta website and request access, then go the github and use the download.sh scrip as shown below.\n"
      ],
      "metadata": {
        "id": "j5YxiWmS30_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/meta-llama/llama/\n",
        "!mv ./llama/download.sh ./\n",
        "!rm -rf llama\n",
        "!bash download.sh\n",
        "!rm -rf LICENSE USE_POLICY.md tokenizer_checklist.chk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTgN2c7aYSDA",
        "outputId": "865888f0-450e-438e-e612-6afbd2e1255c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/47)\u001b[K\rremote: Counting objects:   4% (2/47)\u001b[K\rremote: Counting objects:   6% (3/47)\u001b[K\rremote: Counting objects:   8% (4/47)\u001b[K\rremote: Counting objects:  10% (5/47)\u001b[K\rremote: Counting objects:  12% (6/47)\u001b[K\rremote: Counting objects:  14% (7/47)\u001b[K\rremote: Counting objects:  17% (8/47)\u001b[K\rremote: Counting objects:  19% (9/47)\u001b[K\rremote: Counting objects:  21% (10/47)\u001b[K\rremote: Counting objects:  23% (11/47)\u001b[K\rremote: Counting objects:  25% (12/47)\u001b[K\rremote: Counting objects:  27% (13/47)\u001b[K\rremote: Counting objects:  29% (14/47)\u001b[K\rremote: Counting objects:  31% (15/47)\u001b[K\rremote: Counting objects:  34% (16/47)\u001b[K\rremote: Counting objects:  36% (17/47)\u001b[K\rremote: Counting objects:  38% (18/47)\u001b[K\rremote: Counting objects:  40% (19/47)\u001b[K\rremote: Counting objects:  42% (20/47)\u001b[K\rremote: Counting objects:  44% (21/47)\u001b[K\rremote: Counting objects:  46% (22/47)\u001b[K\rremote: Counting objects:  48% (23/47)\u001b[K\rremote: Counting objects:  51% (24/47)\u001b[K\rremote: Counting objects:  53% (25/47)\u001b[K\rremote: Counting objects:  55% (26/47)\u001b[K\rremote: Counting objects:  57% (27/47)\u001b[K\rremote: Counting objects:  59% (28/47)\u001b[K\rremote: Counting objects:  61% (29/47)\u001b[K\rremote: Counting objects:  63% (30/47)\u001b[K\rremote: Counting objects:  65% (31/47)\u001b[K\rremote: Counting objects:  68% (32/47)\u001b[K\rremote: Counting objects:  70% (33/47)\u001b[K\rremote: Counting objects:  72% (34/47)\u001b[K\rremote: Counting objects:  74% (35/47)\u001b[K\rremote: Counting objects:  76% (36/47)\u001b[K\rremote: Counting objects:  78% (37/47)\u001b[K\rremote: Counting objects:  80% (38/47)\u001b[K\rremote: Counting objects:  82% (39/47)\u001b[K\rremote: Counting objects:  85% (40/47)\u001b[K\rremote: Counting objects:  87% (41/47)\u001b[K\rremote: Counting objects:  89% (42/47)\u001b[K\rremote: Counting objects:  91% (43/47)\u001b[K\rremote: Counting objects:  93% (44/47)\u001b[K\rremote: Counting objects:  95% (45/47)\u001b[K\rremote: Counting objects:  97% (46/47)\u001b[K\rremote: Counting objects: 100% (47/47)\u001b[K\rremote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/34)\u001b[K\rremote: Compressing objects:   5% (2/34)\u001b[K\rremote: Compressing objects:   8% (3/34)\u001b[K\rremote: Compressing objects:  11% (4/34)\u001b[K\rremote: Compressing objects:  14% (5/34)\u001b[K\rremote: Compressing objects:  17% (6/34)\u001b[K\rremote: Compressing objects:  20% (7/34)\u001b[K\rremote: Compressing objects:  23% (8/34)\u001b[K\rremote: Compressing objects:  26% (9/34)\u001b[K\rremote: Compressing objects:  29% (10/34)\u001b[K\rremote: Compressing objects:  32% (11/34)\u001b[K\rremote: Compressing objects:  35% (12/34)\u001b[K\rremote: Compressing objects:  38% (13/34)\u001b[K\rremote: Compressing objects:  41% (14/34)\u001b[K\rremote: Compressing objects:  44% (15/34)\u001b[K\rremote: Compressing objects:  47% (16/34)\u001b[K\rremote: Compressing objects:  50% (17/34)\u001b[K\rremote: Compressing objects:  52% (18/34)\u001b[K\rremote: Compressing objects:  55% (19/34)\u001b[K\rremote: Compressing objects:  58% (20/34)\u001b[K\rremote: Compressing objects:  61% (21/34)\u001b[K\rremote: Compressing objects:  64% (22/34)\u001b[K\rremote: Compressing objects:  67% (23/34)\u001b[K\rremote: Compressing objects:  70% (24/34)\u001b[K\rremote: Compressing objects:  73% (25/34)\u001b[K\rremote: Compressing objects:  76% (26/34)\u001b[K\rremote: Compressing objects:  79% (27/34)\u001b[K\rremote: Compressing objects:  82% (28/34)\u001b[K\rremote: Compressing objects:  85% (29/34)\u001b[K\rremote: Compressing objects:  88% (30/34)\u001b[K\rremote: Compressing objects:  91% (31/34)\u001b[K\rremote: Compressing objects:  94% (32/34)\u001b[K\rremote: Compressing objects:  97% (33/34)\u001b[K\rremote: Compressing objects: 100% (34/34)\u001b[K\rremote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 464 (delta 17), reused 33 (delta 12), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (464/464), 1.12 MiB | 12.70 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n",
            "Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "\n",
            "Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B-chat\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2024-07-01 19:25:04--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.110, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7020 (6.9K) [binary/octet-stream]\n",
            "Saving to: ‘./LICENSE’\n",
            "\n",
            "./LICENSE           100%[===================>]   6.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 19:25:04 (236 MB/s) - ‘./LICENSE’ saved [7020/7020]\n",
            "\n",
            "--2024-07-01 19:25:04--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.110, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4766 (4.7K) [binary/octet-stream]\n",
            "Saving to: ‘./USE_POLICY.md’\n",
            "\n",
            "./USE_POLICY.md     100%[===================>]   4.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 19:25:04 (147 MB/s) - ‘./USE_POLICY.md’ saved [4766/4766]\n",
            "\n",
            "Downloading tokenizer\n",
            "--2024-07-01 19:25:04--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.110, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 499723 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer.model’\n",
            "\n",
            "./tokenizer.model   100%[===================>] 488.01K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-07-01 19:25:05 (7.51 MB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
            "\n",
            "--2024-07-01 19:25:05--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.110, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50 [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer_checklist.chk’\n",
            "\n",
            "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 19:25:05 (2.98 MB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
            "\n",
            "tokenizer.model: OK\n",
            "Downloading llama-2-7b-chat\n",
            "--2024-07-01 19:25:05--  https://download.llamameta.net/llama-2-7b-chat/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.110, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476925163 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/consolidated.00.pth’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]  12.55G  28.1MB/s    in 1m 47s  \n",
            "\n",
            "2024-07-01 19:26:52 (121 MB/s) - ‘./llama-2-7b-chat/consolidated.00.pth’ saved [13476925163/13476925163]\n",
            "\n",
            "--2024-07-01 19:26:52--  https://download.llamameta.net/llama-2-7b-chat/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.69, 18.244.202.110, 18.244.202.122, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [application/json]\n",
            "Saving to: ‘./llama-2-7b-chat/params.json’\n",
            "\n",
            "./llama-2-7b-chat/p 100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 19:26:52 (534 KB/s) - ‘./llama-2-7b-chat/params.json’ saved [102/102]\n",
            "\n",
            "--2024-07-01 19:26:52--  https://download.llamameta.net/llama-2-7b-chat/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoibzlkdmpkcnFkcXUwbm95bGEwaDE1dzVkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTk0Mzk0Nn19fV19&Signature=L5idJ5fteOnNqafKUqTM71WETyx0UmHoeeLZJm3ZhPCiadMA286EgdqdFxL6L-HlfjzVXjZ6fKqERpHRbSAQjFVZVAlNMT4%7EeFCPIT1NrKzCQ1Zt5WaXgbrQmjeFPeYa8ZigKxxLD9X5isEbBC8xQ2-r4Y8sytHKT5GhJEX-zfaiymZegJSOPdh-pH7DAlM81c3mdXOqceCcwddFznI2PEaB-cKcjlJUMcpT7dtFMszFKf6l2Ajq2PBF8h5YPhBNkG1HYdB5zXFNIurjHJOy-WmXBrd957uTBV-KG2es0La0n-3yRsQiJW09gT5jNUjwluF7fFB2FkkObNodAVeaaw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1392284504779997\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.69, 18.244.202.110, 18.244.202.122, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100 [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/checklist.chk’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]     100  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-01 19:26:52 (3.43 MB/s) - ‘./llama-2-7b-chat/checklist.chk’ saved [100/100]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating the architecture"
      ],
      "metadata": {
        "id": "REMnXCWz4WaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Importing everything..."
      ],
      "metadata": {
        "id": "ClefDRdS42cY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zw7Pu7e8NUks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim: int=4096\n",
        "  n_layers: int=32\n",
        "  n_heads: int=32 #number of heads for the queries\n",
        "  n_kv_heads: Optional[int]=None #number of values for the key and value\n",
        "  vocab_size: int = -1 # This will be set when we load the tokenizer\n",
        "  multiple_of: int = 256\n",
        "  ffn_dim_multiplier: Optional[float]=None\n",
        "  norm_eps: float = 1e-5\n",
        "\n",
        "  #Needed for KV cache\n",
        "  max_batch_size: int=32\n",
        "  max_seq_len: int=2048\n",
        "\n",
        "  device: str=None\n",
        "\n",
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0 ):\n",
        "  # as written in the paper, the dimension of the embedding must be even.\n",
        "  assert head_dim % 2 ==0, \"Dimension must be divisible by 2\"\n",
        "  # Build the theta parameters\n",
        "  # According to the formula theta_i = 10000^ (-2(i-1)/dim) for i = [1, 2, ... dim / 2]\n",
        "  # Shape: (head_dim / 2)\n",
        "  theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "  theta = 1.0 / (theta ** (theta_numerator/head_dim)).to(device)\n",
        "  # Construct the positions (the \"m\" parameter)\n",
        "  # shape: (seq_len)\n",
        "  m = torch.arange(seq_len, device=device)\n",
        "  # Multiply each theta by each position using the outer product\n",
        "  # Shape: {Seq_Len} outer_product * (head_dim / 2) -> (seq_len, head_dim / 2)\n",
        "  freqs = torch.outer(m, theta).float()\n",
        "  # we can compute complex numbers in the polar form c = R * exp(i * m * theta), where r = 1 as follows:\n",
        "  freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "  return freqs_complex\n",
        "\n",
        "def apply_rotary_embedding(x: torch.Tensor, freqs_complex: torch.Tensor, device: str)\n",
        "  # (B, Seq_len, H, Head_dim) -> (B,Seq_len, H, head_dim/2)\n",
        "  x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:1], -1, 2))\n",
        "  # (Seq_len, head_dim / 2) -> (1, Seq_len, head_dim / 2)\n",
        "  freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  # (B, Seq_len, H, head_dim / 2) * (1, Seq_len, 1, head_dim / 2) * (B, seq_len, H, Head_Dim / 2)\n",
        "  x_rotated = x_complex * freqs_complex\n",
        "  # (B, seq_len, H, head_dim / 2) -> (B, seq_len, H, head_dim / 2, 2)\n",
        "  x_out = torch.view_as_real(x_rotated)\n",
        "  #(B, seq_len, H, head_dim / 2, 2) -> (B, seq_len, H, head_dim)\n",
        "  x_out = x_out.reshape(*x.shape)\n",
        "\n",
        "  return x_out.type_as(x).to(device)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim: int, eps: float=1e-5) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # the gamma parameter\n",
        "    self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # (B, seq_len, Dim)\n",
        "    # rsqrt: 1/sqrt(x)\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True)) + self.eps\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    #(DIM) * (B, Seq_len, Dim) * (B, seq_len, Dim)\n",
        "    return self.weight * self._norm(x.float()).type_as(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_head=args.n_heads\n",
        "    self.dim = args.dim\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.attention = SelfAttention(args)\n",
        "    self.feed_forward = FeedForward(args)\n",
        "\n",
        "    # Normalization BEFORE the self attention\n",
        "    self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    # Normalization BEFORE the feed forward block\n",
        "    self.ffn_norm = RMSNorm(args.dim, eps= args.norm_eps)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    # (B, seq_len, Dim) + (B, seq_len, Dim) -> (B, seq_len, Dim)\n",
        "    h = x + self.attention.forward(self.attention_norm(x), start_pos,freqs_complex)\n",
        "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, args: ModelArgs) -> None:\n",
        "    super().__init__()\n",
        "    assert args.vocab_size != -1, \"Vocab size must be set\"\n",
        "\n",
        "    self.args = args\n",
        "    self.vocab_size = args.vocab_size\n",
        "    self.n_layers = args.n_layers\n",
        "    self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for _ in range(args.n_layers):\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "\n",
        "    self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_length * 2, device=self.args.device)\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "    # (B, seq_len)\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"only one token at a time can be processed\" #this only works for inference for training you must process more than 1 token at a time and must also remove KV cache\n",
        "\n",
        "    # (B, Seq_Len) -> (B, seq_len, Dim)\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos+seq_len]\n",
        "\n",
        "    # Consecutively apply all the encoder layers\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "\n",
        "    output = self.output(h).float()\n",
        "    return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__qz9L6dqfKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}