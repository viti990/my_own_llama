{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMC4ExwKgQzMb1exkcsOzK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viti990/my_own_llama/blob/main/llamav27b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aviator LLaMA\n",
        "\n",
        "This notebook aims at reconstructing LLAMA 2 7B architecture and use the model weights trained from meta to learn how it is done!\n",
        "\n",
        "Also the aim is to adapt the model with LoRA and QLoRA for PEFT!\n",
        "\n",
        "The fine tuning will be done with aviation regulations from 14 CFR...\n"
      ],
      "metadata": {
        "id": "H57i8tK00XR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Downloading the weights\n",
        "\n",
        "Downloading the weights from meta for 7B-chat (i want the model to be able to respond to questions from the requirements)\n",
        "for this one must go to the meta website and request access, then go the github and use the download.sh scrip as shown below.\n"
      ],
      "metadata": {
        "id": "j5YxiWmS30_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/meta-llama/llama/\n",
        "!mv ./llama/download.sh ./\n",
        "!rm -rf llama\n",
        "!bash download.sh\n",
        "!rm -rf LICENSE USE_POLICY.md tokenizer_checklist.chk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTgN2c7aYSDA",
        "outputId": "976351fc-552b-412c-a9ff-a510fdf9ae11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 464 (delta 17), reused 33 (delta 12), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (464/464), 1.12 MiB | 39.38 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n",
            "Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "\n",
            "Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2024-07-10 21:46:34--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7020 (6.9K) [binary/octet-stream]\n",
            "Saving to: ‘./LICENSE’\n",
            "\n",
            "./LICENSE           100%[===================>]   6.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-10 21:46:34 (326 MB/s) - ‘./LICENSE’ saved [7020/7020]\n",
            "\n",
            "--2024-07-10 21:46:34--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4766 (4.7K) [binary/octet-stream]\n",
            "Saving to: ‘./USE_POLICY.md’\n",
            "\n",
            "./USE_POLICY.md     100%[===================>]   4.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-10 21:46:34 (54.6 MB/s) - ‘./USE_POLICY.md’ saved [4766/4766]\n",
            "\n",
            "Downloading tokenizer\n",
            "--2024-07-10 21:46:34--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 499723 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer.model’\n",
            "\n",
            "./tokenizer.model   100%[===================>] 488.01K  95.3KB/s    in 5.5s    \n",
            "\n",
            "2024-07-10 21:46:41 (87.9 KB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
            "\n",
            "--2024-07-10 21:46:41--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50 [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer_checklist.chk’\n",
            "\n",
            "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-10 21:46:41 (78.2 MB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
            "\n",
            "tokenizer.model: OK\n",
            "Downloading llama-2-7b\n",
            "--2024-07-10 21:46:41--  https://download.llamameta.net/llama-2-7b/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.72, 13.33.88.45, 13.33.88.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476925163 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b/consolidated.00.pth’\n",
            "\n",
            "./llama-2-7b/consol 100%[===================>]  12.55G   321MB/s    in 42s     \n",
            "\n",
            "2024-07-10 21:47:24 (306 MB/s) - ‘./llama-2-7b/consolidated.00.pth’ saved [13476925163/13476925163]\n",
            "\n",
            "--2024-07-10 21:47:24--  https://download.llamameta.net/llama-2-7b/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.45, 13.33.88.62, 13.33.88.113, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [application/json]\n",
            "Saving to: ‘./llama-2-7b/params.json’\n",
            "\n",
            "./llama-2-7b/params 100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-10 21:47:26 (107 MB/s) - ‘./llama-2-7b/params.json’ saved [102/102]\n",
            "\n",
            "--2024-07-10 21:47:26--  https://download.llamameta.net/llama-2-7b/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZGszZzhhenp4M2N1aXFiaGV4MzA5ZGp6IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDcyOTUyN319fV19&Signature=n5EXkvU39BOn%7EzT8Sob0t9o%7EpRXA7tzeIXa9YQ36VnRfjo4UezTlvRqgMz68NIonZVz0P6X8-cDiRXHM0svPI5tuuH0VkZgjdCaBvCxr%7EDUdsRBW2BxItnK-d8%7ELUW%7EsixBJ9ApgCoPHmCJXM9QGoyptMq6gCAPmUiuVzeRcNPzvWSZYoVFuAYBnzZhalaHsoVuI6uIwHuA5%7EaaorhwARBhYaWCcInd708yI%7ETiYqor6K29JGXHJ%7EKSUpk9uhB4oQvOjc4P2aiUqo%7E%7E9X4AXxz1YSPJy30JnP6AO6e3Cvxbj1Nv1nwvgZTWU%7EQ04-abMZ6X6LdvtC7IqmwmAYUX1yA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1595127994388383\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 13.33.88.45, 13.33.88.62, 13.33.88.113, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|13.33.88.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100 [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b/checklist.chk’\n",
            "\n",
            "./llama-2-7b/checkl 100%[===================>]     100  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-10 21:47:26 (11.6 MB/s) - ‘./llama-2-7b/checklist.chk’ saved [100/100]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating the architecture"
      ],
      "metadata": {
        "id": "REMnXCWz4WaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Importing everything..."
      ],
      "metadata": {
        "id": "ClefDRdS42cY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zw7Pu7e8NUks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 4096\n",
        "    n_layers: int = 32\n",
        "    n_heads: int = 32\n",
        "    n_kv_heads: Optional[int] = None\n",
        "    vocab_size: int = -1  # Later set in the build method\n",
        "    multiple_of: int = 256\n",
        "    ffn_dim_multiplier: Optional[float] = None\n",
        "    norm_eps: float = 1e-5\n",
        "\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 2048\n",
        "\n",
        "    device: str = None\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x: torch.Tensor):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.weight * self._norm(x.float()).type_as(x)\n",
        "\n",
        "\n",
        "def precompute_theta_pos_frequencies(\n",
        "    head_dim: int, seq_len: int, device: str, theta: float = 10000.0\n",
        "):\n",
        "    assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
        "    theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)\n",
        "    m = torch.arange(seq_len, device=device)\n",
        "    freqs = torch.outer(m, theta).float()\n",
        "    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_complex\n",
        "\n",
        "\n",
        "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "\n",
        "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "    x_rotated = x_complex * freqs_complex\n",
        "    x_out = torch.view_as_real(x_rotated)\n",
        "    x_out = x_out.reshape(*x.shape)\n",
        "    return x_out.type_as(x).to(device)\n",
        "\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        self.n_heads_q = args.n_heads\n",
        "\n",
        "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        self.cache_k = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
        "        )\n",
        "        self.cache_v = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "        batch_size, seq_len, _ = x.shape  # (B, 1, Dim)\n",
        "\n",
        "        xq = self.wq(x)\n",
        "        xk = self.wk(x)\n",
        "        xv = self.wv(x)\n",
        "\n",
        "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
        "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
        "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
        "\n",
        "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
        "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
        "\n",
        "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
        "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
        "\n",
        "        keys = repeat_kv(keys, self.n_rep)\n",
        "        values = repeat_kv(values, self.n_rep)\n",
        "\n",
        "        xq = xq.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "        output = torch.matmul(scores, values)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        return self.wo(output)  # (B, 1, Dim) -> (B, 1, Dim)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = 4 * args.dim\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if args.ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
        "        hidden_dim = args.multiple_of * (\n",
        "            (hidden_dim + args.multiple_of - 1) // args.multiple_of\n",
        "        )\n",
        "\n",
        "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
        "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        swish = F.silu(self.w1(x))\n",
        "        x_V = self.w3(x)\n",
        "        x = swish * x_V\n",
        "        x = self.w2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.attention = SelfAttention(args)\n",
        "        self.feed_forward = FeedForward(args)\n",
        "\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
        "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        assert args.vocab_size != -1, \"Vocab size must be set\"\n",
        "\n",
        "        self.args = args\n",
        "        self.vocab_size = args.vocab_size\n",
        "        self.n_layers = args.n_layers\n",
        "        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for layer_id in range(args.n_layers):\n",
        "            self.layers.append(EncoderBlock(args))\n",
        "\n",
        "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "        self.freqs_complex = precompute_theta_pos_frequencies(\n",
        "            self.args.dim // self.args.n_heads,\n",
        "            self.args.max_seq_len * 2,\n",
        "            device=self.args.device,\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "\n",
        "        batch_size, seq_len = tokens.shape\n",
        "        assert seq_len == 1, \"Only one token at a time can be processed\"\n",
        "\n",
        "        h = self.tok_embeddings(tokens)\n",
        "\n",
        "        freqs_complex = self.freqs_complex[start_pos : start_pos + seq_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, start_pos, freqs_complex)\n",
        "        h = self.norm(h)\n",
        "        output = self.output(h).float()\n",
        "        return output"
      ],
      "metadata": {
        "id": "PJ6VbwmSZ4BW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim: int=4096\n",
        "  n_layers: int=32\n",
        "  n_heads: int=32 #number of heads for the queries\n",
        "  n_kv_heads: Optional[int]=None #number of values for the key and value\n",
        "  vocab_size: int = -1 # This will be set when we load the tokenizer\n",
        "  multiple_of: int = 256\n",
        "  ffn_dim_multiplier: Optional[float]=None\n",
        "  norm_eps: float = 1e-5\n",
        "\n",
        "  #Needed for KV cache\n",
        "  max_batch_size: int=32\n",
        "  max_seq_len: int=2048\n",
        "\n",
        "  device: str=None\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "      super().__init__()\n",
        "      self.eps = eps\n",
        "      # the gamma parameter\n",
        "      self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x: torch.Tensor):\n",
        "      # (B, seq_len, dim) * (B, seq_len, 1) = (B, seq_len, dim)\n",
        "      # rsqrt: 1/sqrt(x)\n",
        "      return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "      #(dim) * (B, Seq_len, dim) * (B, seq_len, dim)\n",
        "      return self.weight * self._norm(x.float()).type_as(x)\n",
        "\n",
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0 ):\n",
        "  # as written in the paper, the dimension of the embedding must be even.\n",
        "  assert head_dim % 2 ==0, \"Dimension must be divisible by 2\"\n",
        "  # Build the theta parameters\n",
        "  # According to the formula theta_i = 10000^ (-2(i-1)/dim) for i = [1, 2, ... dim / 2]\n",
        "  # Shape: (head_dim / 2)\n",
        "  theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "  theta = 1.0 / (theta ** (theta_numerator/head_dim)).to(device)\n",
        "  # Construct the positions (the \"m\" parameter)\n",
        "  # shape: (seq_len)\n",
        "  m = torch.arange(seq_len, device=device)\n",
        "  # Multiply each theta by each position using the outer product\n",
        "  # Shape: {Seq_Len} outer_product * (head_dim / 2) -> (seq_len, head_dim / 2)\n",
        "  freqs = torch.outer(m, theta).float()\n",
        "  # we can compute complex numbers in the polar form c = R * exp(i * m * theta), where r = 1 as follows:\n",
        "  freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "  return freqs_complex\n",
        "\n",
        "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "  # (B, Seq_len, H, Head_dim) -> (B,Seq_len, H, head_dim/2)\n",
        "  x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "  # (Seq_len, head_dim / 2) -> (1, Seq_len, 1, head_dim / 2)\n",
        "  freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  # (B, Seq_len, H, head_dim / 2) * (1, Seq_len, 1, head_dim / 2) = (B, seq_len, H, Head_Dim / 2)\n",
        "  x_rotated = x_complex * freqs_complex\n",
        "  # (B, seq_len, H, head_dim / 2) -> (B, seq_len, H, head_dim / 2, 2)\n",
        "  x_out = torch.view_as_real(x_rotated)\n",
        "  # (B, seq_len, H, head_dim / 2, 2) -> (B, seq_len, H, head_dim)\n",
        "  x_out = x_out.reshape(*x.shape)\n",
        "\n",
        "  return x_out.type_as(x).to(device)\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "  batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
        "  if n_rep == 1:\n",
        "     return x\n",
        "  # (B, seq-len, n_kv_heads, 1, head_dim)\n",
        "  return (\n",
        "      x[:, :, :, None, :]\n",
        "      .expand(batch_size,seq_len, n_kv_heads, n_rep, head_dim)\n",
        "      .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "  )\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    # Indicates the number of heads fot the key and values\n",
        "    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    # Indicates the number of heads for the queries\n",
        "    self.n_heads_q = args.n_heads\n",
        "    # Indicates how many times the heads of keys and values should be repeated to match the head of the queries\n",
        "    self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "    # Indicates the dimension of each head\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias = False)\n",
        "    self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias = False)\n",
        "    self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias = False)\n",
        "    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim,  bias = False)\n",
        "\n",
        "    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    batch_size, seq_len, _ = x.shape #(B, 1, dim)\n",
        "\n",
        "    # Apply the wq, wk, wv matrices to queries, keys and values\n",
        "    # (B, 1, dim) -> (B, 1, H_Q * head_dim)\n",
        "    xq = self.wq(x)\n",
        "\n",
        "    # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
        "    xk = self.wk(x)\n",
        "    xv = self.wv(x)\n",
        "\n",
        "    # (B, 1, H_Q * head_dim) -> (B, 1, H_Q, head_dim)\n",
        "    xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
        "\n",
        "    # (B, 1, H_KV * head_dim) -> (B, 1, H_KV, head_dim)\n",
        "    xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "    xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "    # Does not change the shape of the tensors\n",
        "    xq = apply_rotary_embeddings(xq, freqs_complex, device = x.device)\n",
        "    xk = apply_rotary_embeddings(xk, freqs_complex, device = x.device)\n",
        "\n",
        "    # Replace the entry in the cache fot this token\n",
        "    self.cache_k[:batch_size, start_pos:start_pos+seq_len] = xk\n",
        "    self.cache_v[:batch_size, start_pos:start_pos+seq_len] = xv\n",
        "\n",
        "    # Retrieve all the cached keys and values so far\n",
        "    # (B, seq_len_KV, H_KV, head_dim)\n",
        "    keys = self.cache_k[:batch_size, :start_pos+seq_len]\n",
        "    values = self.cache_v[:batch_size, :start_pos+seq_len]\n",
        "\n",
        "    # Repeat the heads of the K and V to reach the number of heads of the queries\n",
        "    keys = repeat_kv(keys, self.n_rep)\n",
        "    values = repeat_kv(values, self.n_rep)\n",
        "\n",
        "    # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
        "    xq = xq.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # (B, H_Q, 1, seq_len) @ (B, H_Q, seq_len_kv, head_dim) -> (B, H_Q, 1, head_dim)\n",
        "    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "    # (B, H_Q, 1, head_dim) -> (B, 1, H_Q, head_dim) -> (B, 1, dim)\n",
        "    output = torch.matmul(scores, values)\n",
        "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "\n",
        "    return self.wo(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    hidden_dim = 4 * args.dim\n",
        "    hidden_dim = int(2 * hidden_dim / 3)\n",
        "    if args.ffn_dim_multiplier is not None:\n",
        "      hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
        "    # round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
        "    hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "\n",
        "    self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "    self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
        "    self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    swish = F.silu(self.w1(x))\n",
        "    x_V = self.w3(x)\n",
        "    x = swish * x_V\n",
        "    x = self.w2(x)\n",
        "    return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads=args.n_heads\n",
        "    self.dim = args.dim\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.attention = SelfAttention(args)\n",
        "    self.feed_forward = FeedForward(args)\n",
        "\n",
        "    # Normalization BEFORE the self attention\n",
        "    self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    # Normalization BEFORE the feed forward block\n",
        "    self.ffn_norm = RMSNorm(args.dim, eps= args.norm_eps)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    # (B, seq_len, Dim) + (B, seq_len, Dim) -> (B, seq_len, Dim)\n",
        "    h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
        "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    assert args.vocab_size != -1, \"Vocab size must be set\"\n",
        "\n",
        "    self.args = args\n",
        "    self.vocab_size = args.vocab_size\n",
        "    self.n_layers = args.n_layers\n",
        "    self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for layer_id in range(args.n_layers):\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "\n",
        "    self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "    # (B, seq_len)\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"only one token at a time can be processed\" #this only works for inference for training you must process more than 1 token at a time and must also remove KV cache\n",
        "\n",
        "    # (B, Seq_Len) -> (B, seq_len, Dim)\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos+seq_len]\n",
        "\n",
        "    # Consecutively apply all the encoder layers\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "\n",
        "    output = self.output(h).float()\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "__qz9L6dqfKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from model import ModelArgs, Transformer\n",
        "\n",
        "class LLaMA:\n",
        "\n",
        "  def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.args = model_args\n",
        "\n",
        "  @staticmethod\n",
        "  def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
        "    prev_time = time.time()\n",
        "    if load_model:\n",
        "      checkpoints = sorted(Path(checkpoints_dir).glob('*.pth'))\n",
        "      assert len(checkpoints) > 0, \"No checkpoints files found\"\n",
        "      chk_path = checkpoints[0]\n",
        "      print(f\"Loading checkpoint {chk_path}\")\n",
        "      checkpoint = torch.load(chk_path, map_location=\"cpu\")\n",
        "      print(f\"Loaded checkpoint in {(time.time() - prev_time):.2f}s\")\n",
        "      prev_time = time.time()\n",
        "\n",
        "    with open (Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
        "      params = json.loads(f.read())\n",
        "\n",
        "    model_args: ModelArgs = ModelArgs(\n",
        "      max_seq_len=max_seq_len,\n",
        "      max_batch_size=max_batch_size,\n",
        "      device=device,\n",
        "      **params\n",
        "    )\n",
        "\n",
        "    tokenizer = SentencePieceProcessor()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "    model_args.vocab_size = tokenizer.vocab_size()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "      torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "    else:\n",
        "      torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
        "\n",
        "    model = Transformer(model_args).to(device)\n",
        "    if load_model:\n",
        "      del checkpoint[\"rope.freqs\"]\n",
        "      model.load_state_dict(checkpoint, strict=True)\n",
        "      print(f\"loaded state dict in {(time.time() - prev_time):.2f}s\")\n",
        "\n",
        "    # print(f\"Loaded in {(time.time() - prev_time):.2f}s\")\n",
        "    return LLaMA(model, tokenizer, model_args)\n",
        "\n",
        "  def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
        "    if max_gen_len is None:\n",
        "      max_gen_len = self.args.max_seq_len - 1\n",
        "    # Convert each prompt into tokens\n",
        "    prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
        "    # Make sure the batch size is not too large\n",
        "    batch_size = len(prompt_tokens)\n",
        "    assert batch_size <= self.args.max_batch_size\n",
        "    max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
        "    # make sure the prompt length is not larger than the maximum seq length\n",
        "    assert max_prompt_len <= self.args.max_seq_len\n",
        "    total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
        "\n",
        "    # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
        "    pad_id = self.tokenizer.pad_id()\n",
        "    tokens = torch.full((batch_size, total_len), pad_id, dtype = torch.long, device=device)\n",
        "    for k, t in enumerate(prompt_tokens):\n",
        "      # Populate the initial tokens with the prompt token, False otherwise\n",
        "      tokens[k, :len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
        "\n",
        "    eos_reached = torch.tensor([False] * batch_size, device=device)\n",
        "    prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, false otherwise\n",
        "    for cur_pos in tqdm(range(1, total_len), desc=\"Generating...\"):\n",
        "      with torch.inference_mode():\n",
        "        logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
        "      if temperature > 0 :\n",
        "        # The temperature  is applied BEFORE the softmax\n",
        "        probs = torch.softmax(logits[:, -1] / temperature, dim = -1)\n",
        "        next_token = self._sample_top_p(probs, top_p)\n",
        "      else:\n",
        "        # Greedly select the token with the maximum probability\n",
        "        next_token = torch.argmax(logits[:, -1], dim = -1)\n",
        "      next_token = next_token.reshape(-1)\n",
        "      # only replace the token if it is a padding token\n",
        "      next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "      tokens[:, cur_pos] = next_token\n",
        "      # EOS is reached only if we found an EOS token for a padding position\n",
        "      eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id())\n",
        "      if all(eos_reached):\n",
        "        break\n",
        "    out_tokens=[]\n",
        "    out_text=[]\n",
        "    for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "      # Cut to the EOS token, if present\n",
        "      if self.tokenizer.eos_id() in current_prompt_tokens:\n",
        "        eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id())\n",
        "        current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "      out_tokens.append(current_prompt_tokens)\n",
        "      out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
        "    return (out_tokens, out_text)\n",
        "\n",
        "  def _sample_top_p(self, probs, p):\n",
        "    probs_sort, probs_idx = torch.sort(probs,dim = -1,descending = True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort.div_(probs_sort.sum(-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "\n",
        "    return next_token\n"
      ],
      "metadata": {
        "id": "HgBUJKJphoBl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "allow_cuda = True\n",
        "device = \"cuda\" if torch.cuda.is_available() and allow_cuda else \"cpu\"\n",
        "\n",
        "prompts = [\n",
        "    \" Simply put, the theory of relativity states that\",\n",
        "    \"If google was an italian company founded in Milan, it would\",\n",
        "    # Few shot prompt\n",
        "    \"\"\"Translate English to French:\n",
        "\n",
        "    sea otter => loutre de mer\n",
        "    peppermint => menthe poivrée\n",
        "    plush giraffe => girafe peluche\n",
        "    cheese omelette =>\"\"\",\n",
        "    # Zero shot prompt\n",
        "    \"\"\" Tell me if the following person is actually Doraemon disguised as human:\n",
        "    Name: Umar Jamil\n",
        "    Decision:\"\"\"]\n",
        "\n",
        "model = LLaMA.build(\n",
        "    checkpoints_dir=\"llama-2-7b\",\n",
        "    tokenizer_path=\"./tokenizer.model\",\n",
        "    load_model=True,\n",
        "    max_seq_len=1024,\n",
        "    max_batch_size=len(prompts),\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhjvltaanBkD",
        "outputId": "05092c3f-008b-45df-b14e-947da4772646"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint llama-2-7b/consolidated.00.pth\n",
            "Loaded checkpoint in 7.87s\n",
            "loaded state dict in 6.86s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference the model\n",
        "out_tokens, out_text = (model.text_completion(prompts, max_gen_len=64))\n",
        "assert len(out_text) == len(prompts)\n",
        "for i in range(len(out_text)):\n",
        "  print(f\"{out_text[i]}\")\n",
        "  print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "XMotm-8e112v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0993e28-695e-4fd8-cfce-0e180ea808ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating...: 100%|██████████| 116/116 [00:05<00:00, 22.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Simply put, the theory of relativity states that the laws of physics are the same in all inertial frames of reference.$ In other words, the laws of physics are the same for all observers who are not accelerating relative to each other.\n",
            " The special theory of relativity is a theory of the laws of physics that apply to all observers who are not accelerating relative to each other. The theory is based on the principle of relativity, which states that the laws of physics are the same in all inertial frames of reference.\n",
            "\n",
            "--------------------------------------------------\n",
            "If google was an italian company founded in Milan, it would be a big company.\n",
            "I'm not sure if you have noticed, but there is a new Italian company that is trying to make a big difference in the world.\n",
            "It's called Google and it's a company that has been around for a while.\n",
            "Google is a big company.\n",
            "I know that you're not going to be able to tell me that you don't know what Google is.\n",
            "You're not going to want to know what it is.\n",
            "You\n",
            "--------------------------------------------------\n",
            "Translate English to French:\n",
            "\n",
            "    sea otter => loutre de mer\n",
            "    peppermint => menthe poivrée\n",
            "    plush giraffe => girafe peluche\n",
            "    cheese omelette => omelette aux fromage\n",
            "\n",
            "Translate French to English:\n",
            "\n",
            "    loutre de mer => sea otter\n",
            "    menthe poivrée => peppermint\n",
            "    girafe peluche => plush giraffe\n",
            "    omelette aux fromage => cheese o\n",
            "--------------------------------------------------\n",
            " Tell me if the following person is actually Doraemon disguised as human:\n",
            "    Name: Umar Jamil\n",
            "    Decision: A) Yes, it is Doraemon disguised as human.\n",
            "    Reason: Doraemon has a time machine and the only way to tell if it is Doraemon is to see if he has a time machine.\n",
            "    B) No, it is not Doraemon disguised as human.\n",
            "    Reason: There is no way to tell if it is Doraemon dis\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcxCPg55x6nJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}