{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMwgs6ChB8WMyiUlHo0mV7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viti990/my_own_llama/blob/main/llamav27b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aviator LLaMA\n",
        "\n",
        "This notebook aims at reconstructing LLAMA 2 7B architecture and use the model weights trained from meta to learn how it is done!\n",
        "\n",
        "Also the aim is to adapt the model with LoRA and QLoRA for PEFT!\n",
        "\n",
        "The fine tuning will be done with aviation regulations from 14 CFR...\n"
      ],
      "metadata": {
        "id": "H57i8tK00XR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Downloading the weights\n",
        "\n",
        "Downloading the weights from meta for 7B-chat (i want the model to be able to respond to questions from the requirements)\n",
        "for this one must go to the meta website and request access, then go the github and use the download.sh scrip as shown below.\n"
      ],
      "metadata": {
        "id": "j5YxiWmS30_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/meta-llama/llama/\n",
        "!mv ./llama/download.sh ./\n",
        "!rm -rf llama\n",
        "!bash download.sh\n",
        "!rm -rf LICENSE USE_POLICY.md tokenizer_checklist.chk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTgN2c7aYSDA",
        "outputId": "b3ce7ef2-95af-44fd-8d23-a66bda6b6fbd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 464 (delta 17), reused 33 (delta 12), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (464/464), 1.12 MiB | 13.28 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n",
            "Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "\n",
            "Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2024-07-09 18:40:33--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.69, 18.244.202.48, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7020 (6.9K) [binary/octet-stream]\n",
            "Saving to: ‘./LICENSE’\n",
            "\n",
            "./LICENSE           100%[===================>]   6.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-09 18:40:33 (699 MB/s) - ‘./LICENSE’ saved [7020/7020]\n",
            "\n",
            "--2024-07-09 18:40:33--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.69, 18.244.202.48, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4766 (4.7K) [binary/octet-stream]\n",
            "Saving to: ‘./USE_POLICY.md’\n",
            "\n",
            "./USE_POLICY.md     100%[===================>]   4.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-09 18:40:33 (1006 MB/s) - ‘./USE_POLICY.md’ saved [4766/4766]\n",
            "\n",
            "Downloading tokenizer\n",
            "--2024-07-09 18:40:33--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.69, 18.244.202.48, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 499723 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer.model’\n",
            "\n",
            "./tokenizer.model   100%[===================>] 488.01K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-07-09 18:40:33 (8.69 MB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
            "\n",
            "--2024-07-09 18:40:33--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.69, 18.244.202.48, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50 [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer_checklist.chk’\n",
            "\n",
            "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-09 18:40:34 (44.8 MB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
            "\n",
            "tokenizer.model: OK\n",
            "Downloading llama-2-7b\n",
            "--2024-07-09 18:40:34--  https://download.llamameta.net/llama-2-7b/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.69, 18.244.202.48, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476925163 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b/consolidated.00.pth’\n",
            "\n",
            "./llama-2-7b/consol 100%[===================>]  12.55G   285MB/s    in 49s     \n",
            "\n",
            "2024-07-09 18:41:23 (263 MB/s) - ‘./llama-2-7b/consolidated.00.pth’ saved [13476925163/13476925163]\n",
            "\n",
            "--2024-07-09 18:41:23--  https://download.llamameta.net/llama-2-7b/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.69, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [application/json]\n",
            "Saving to: ‘./llama-2-7b/params.json’\n",
            "\n",
            "./llama-2-7b/params 100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-09 18:41:23 (10.4 MB/s) - ‘./llama-2-7b/params.json’ saved [102/102]\n",
            "\n",
            "--2024-07-09 18:41:23--  https://download.llamameta.net/llama-2-7b/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZWNmZDIwYm9xaXQwYmlhYWxycno1NjRpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDYzNDk0M319fV19&Signature=hFxxtS1sJMGmH3uW%7Eti%7EM7mUiVOydtiPzTG3vmFowlCvgwOEfPpXmBIvyqeVVJyiGbSEtX0GlFZtg6A8b93HQ%7Ecgqyea8-q2OvEleKUXp0ke3RT4OSe-3qtfySSdQNh1voO2ycLSkyBZRZjpTE32jkXcPtNUoODQf4glV-bPTZGcoMSH3VByvTjKJsCp8gvRm91OHv37DET2aFI4wr6wFJAChpqOrTWiTtQI4VksPp77AOWWg7vDOwj2yCmIg-hc-k%7EEJIFmOMum8-C1DkVDt2D1AQT-1JSMVn7I1EVClDZD2Ax2q3-A176S5JgwLSTpXYwV5hoXlULYyTvK4GrVbw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1488746315103123\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 18.244.202.122, 18.244.202.48, 18.244.202.69, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|18.244.202.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100 [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b/checklist.chk’\n",
            "\n",
            "./llama-2-7b/checkl 100%[===================>]     100  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-07-09 18:41:23 (116 KB/s) - ‘./llama-2-7b/checklist.chk’ saved [100/100]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating the architecture"
      ],
      "metadata": {
        "id": "REMnXCWz4WaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Importing everything..."
      ],
      "metadata": {
        "id": "ClefDRdS42cY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zw7Pu7e8NUks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim: int=4096\n",
        "  n_layers: int=32\n",
        "  n_heads: int=32 #number of heads for the queries\n",
        "  n_kv_heads: Optional[int]=None #number of values for the key and value\n",
        "  vocab_size: int = -1 # This will be set when we load the tokenizer\n",
        "  multiple_of: int = 256\n",
        "  ffn_dim_multiplier: Optional[float]=None\n",
        "  norm_eps: float = 1e-5\n",
        "\n",
        "  #Needed for KV cache\n",
        "  max_batch_size: int=32\n",
        "  max_seq_len: int=2048\n",
        "\n",
        "  device: str=None\n",
        "\n",
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0 ):\n",
        "  # as written in the paper, the dimension of the embedding must be even.\n",
        "  assert head_dim % 2 ==0, \"Dimension must be divisible by 2\"\n",
        "  # Build the theta parameters\n",
        "  # According to the formula theta_i = 10000^ (-2(i-1)/dim) for i = [1, 2, ... dim / 2]\n",
        "  # Shape: (head_dim / 2)\n",
        "  theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "  theta = 1.0 / (theta ** (theta_numerator/head_dim)).to(device)\n",
        "  # Construct the positions (the \"m\" parameter)\n",
        "  # shape: (seq_len)\n",
        "  m = torch.arange(seq_len, device=device)\n",
        "  # Multiply each theta by each position using the outer product\n",
        "  # Shape: {Seq_Len} outer_product * (head_dim / 2) -> (seq_len, head_dim / 2)\n",
        "  freqs = torch.outer(m, theta).float()\n",
        "  # we can compute complex numbers in the polar form c = R * exp(i * m * theta), where r = 1 as follows:\n",
        "  freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "  return freqs_complex\n",
        "\n",
        "def apply_rotary_embedding(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "  # (B, Seq_len, H, Head_dim) -> (B,Seq_len, H, head_dim/2)\n",
        "  x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "  # (Seq_len, head_dim / 2) -> (1, Seq_len, 1, head_dim / 2)\n",
        "  freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  # (B, Seq_len, H, head_dim / 2) * (1, Seq_len, 1, head_dim / 2) = (B, seq_len, H, Head_Dim / 2)\n",
        "  x_rotated = x_complex * freqs_complex\n",
        "  # (B, seq_len, H, head_dim / 2) -> (B, seq_len, H, head_dim / 2, 2)\n",
        "  x_out = torch.view_as_real(x_rotated)\n",
        "  # (B, seq_len, H, head_dim / 2, 2) -> (B, seq_len, H, head_dim)\n",
        "  x_out = x_out.reshape(*x.shape)\n",
        "\n",
        "  return x_out.type_as(x).to(device)\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "  batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
        "  if n_rep == 1:\n",
        "     return x\n",
        "  else:\n",
        "    # (B, seq-len, n_kv_heads, 1, head_dim)\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(batch_size,seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim: int, eps: float = 1e-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # the gamma parameter\n",
        "    self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self, x: torch.Tensor):\n",
        "    # (B, seq_len, dim) * (B, seq_len, 1) = (B, seq_len, dim)\n",
        "    # rsqrt: 1/sqrt(x)\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    #(dim) * (B, Seq_len, dim) * (B, seq_len, dim)\n",
        "    return self.weight * self._norm(x.float()).type_as(x)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    # Indicates the number of heads fot the key and values\n",
        "    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    # Indicates the number of heads for the queries\n",
        "    self.n_heads_q = args.n_heads\n",
        "    # Indicates how many times the heads of keys and values should be repeated to match the head of the queries\n",
        "    self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "    # Indicates the dimension of each head\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias = False)\n",
        "    self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias = False)\n",
        "    self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias = False)\n",
        "    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim,  bias = False)\n",
        "\n",
        "    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    batch_size, seq_len, _ = x.shape #(B, 1, dim)\n",
        "\n",
        "    # Apply the wq, wk, wv matrices to queries, keys and values\n",
        "    # (B, 1, dim) -> (B, 1, H_Q * head_dim)\n",
        "    xq = self.wq(x)\n",
        "\n",
        "    # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
        "    xk = self.wk(x)\n",
        "    xv = self.wv(x)\n",
        "\n",
        "    # (B, 1, H_Q * head_dim) -> (B, 1, H_Q, head_dim)\n",
        "    xq = xq.view(batch_size, 1, self.n_heads_q, self.head_dim)\n",
        "\n",
        "    # (B, 1, H_KV * head_dim) -> (B, 1, H_KV, head_dim)\n",
        "    xk = xk.view(batch_size, 1, self.n_kv_heads, self.head_dim)\n",
        "    xv = xv.view(batch_size, 1, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "    # Does not change the shape of the tensors\n",
        "    xq = apply_rotary_embedding(xq, freqs_complex, device = x.device)\n",
        "    xk = apply_rotary_embedding(xk, freqs_complex, device = x.device)\n",
        "\n",
        "    # Replace the entry in the cache fot this token\n",
        "    self.cache_k[:batch_size, start_pos:start_pos+seq_len] = xk\n",
        "    self.cache_v[:batch_size, start_pos:start_pos+seq_len] = xv\n",
        "\n",
        "    # Retrieve all the cached keys and values so far\n",
        "    # (B, seq_len_KV, H_KV, head_dim)\n",
        "    keys = self.cache_k[:batch_size, 0:start_pos+seq_len]\n",
        "    values = self.cache_v[:batch_size, 0:start_pos+seq_len]\n",
        "\n",
        "    # Repeat the heads of the K and V to reach the number of heads of the queries\n",
        "    keys = repeat_kv(keys, self.n_rep)\n",
        "    values = repeat_kv(values, self.n_rep)\n",
        "\n",
        "    # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
        "    xq = xq.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # (B, H_Q, 1, seq_len) @ (B, H_Q, seq_len_kv, head_dim) -> (B, H_Q, 1, head_dim)\n",
        "    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "    scores = F.softmax(scores, dim=-1).type_as(xq)\n",
        "\n",
        "    # (B, H_Q, 1, head_dim) -> (B, 1, H_Q, head_dim) -> (B, 1, dim)\n",
        "    output = torch.matmul(scores, values)\n",
        "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "\n",
        "    return self.wo(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    hidden_dim = 4 * args.dim\n",
        "    hidden_dim = int(2 * hidden_dim / 3)\n",
        "    if args.ffn_dim_multiplier is not None:\n",
        "      hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
        "    # round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
        "    hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "\n",
        "    self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "    self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
        "    self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    swish = F.silu(self.w1(x))\n",
        "    x_V = self.w3(x)\n",
        "    x = swish * x_V\n",
        "    x= self.w2(x)\n",
        "    return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_head=args.n_heads\n",
        "    self.dim = args.dim\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.attention = SelfAttention(args)\n",
        "    self.feed_forward = FeedForward(args)\n",
        "\n",
        "    # Normalization BEFORE the self attention\n",
        "    self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    # Normalization BEFORE the feed forward block\n",
        "    self.ffn_norm = RMSNorm(args.dim, eps= args.norm_eps)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    # (B, seq_len, Dim) + (B, seq_len, Dim) -> (B, seq_len, Dim)\n",
        "    h = x + self.attention.forward(self.attention_norm(x), start_pos,freqs_complex)\n",
        "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, args: ModelArgs) -> None:\n",
        "    super().__init__()\n",
        "    assert args.vocab_size != -1, \"Vocab size must be set\"\n",
        "\n",
        "    self.args = args\n",
        "    self.vocab_size = args.vocab_size\n",
        "    self.n_layers = args.n_layers\n",
        "    self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for _ in range(args.n_layers):\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "\n",
        "    self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "    # (B, seq_len)\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"only one token at a time can be processed\" #this only works for inference for training you must process more than 1 token at a time and must also remove KV cache\n",
        "\n",
        "    # (B, Seq_Len) -> (B, seq_len, Dim)\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos+seq_len]\n",
        "\n",
        "    # Consecutively apply all the encoder layers\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "\n",
        "    output = self.output(h).float()\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "__qz9L6dqfKN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from model import ModelArgs, Transformer\n",
        "\n",
        "class LLaMA:\n",
        "\n",
        "  def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.args = model_args\n",
        "\n",
        "  @staticmethod\n",
        "  def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
        "    prev_time = time.time()\n",
        "    if load_model:\n",
        "      checkpoints = sorted(Path(checkpoints_dir).glob('*.pth'))\n",
        "      assert len(checkpoints) > 0, \"No checkpoints files found\"\n",
        "      chk_path = checkpoints[0]\n",
        "      print(f\"Loading checkpoint {chk_path}\")\n",
        "      checkpoint = torch.load(chk_path, map_location=\"cpu\")\n",
        "      print(f\"Loaded checkpoint in {(time.time() - prev_time):.2f}s\")\n",
        "      prev_time = time.time()\n",
        "\n",
        "    with open (Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
        "      params = json.loads(f.read())\n",
        "\n",
        "    model_args: ModelArgs = ModelArgs(\n",
        "      max_seq_len=max_seq_len,\n",
        "      max_batch_size=max_batch_size,\n",
        "      device=device,\n",
        "      **params\n",
        "    )\n",
        "\n",
        "    tokenizer = SentencePieceProcessor()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "    model_args.vocab_size = tokenizer.vocab_size()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "      torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "    else:\n",
        "      torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
        "\n",
        "    model = Transformer(model_args).to(device)\n",
        "    if load_model:\n",
        "      del checkpoint[\"rope.freqs\"]\n",
        "      model.load_state_dict(checkpoint, strict=True)\n",
        "      print(f\"loaded state dict in {(time.time() - prev_time):.2f}s\")\n",
        "\n",
        "    return LLaMA(model, tokenizer, model_args)\n",
        "\n",
        "  def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
        "    if max_gen_len is None:\n",
        "      max_gen_len = self.args.max_seq_len - 1\n",
        "    # Convert each prompt into tokens\n",
        "    prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
        "    # Make sure the batch size is not too large\n",
        "    batch_size = len(prompt_tokens)\n",
        "    assert batch_size <= self.args.max_batch_size\n",
        "    max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
        "    # make sure the prompt length is not larger than the maximum seq length\n",
        "    assert max_prompt_len <= self.args.max_seq_len\n",
        "    total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
        "\n",
        "    # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
        "    pad_id = self.tokenizer.pad_id()\n",
        "    tokens = torch.full((batch_size, total_len), pad_id, dtype = torch.long, device=device)\n",
        "    for k, t in enumerate(prompt_tokens):\n",
        "      # Populate the initial tokens with the prompt token, False otherwise\n",
        "      tokens[k, :len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
        "\n",
        "    eos_reached = torch.tensor([False] * batch_size, device=device)\n",
        "    prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, false otherwise\n",
        "    for cur_pos in tqdm(range(1, total_len), desc=\"Generating...\"):\n",
        "      with torch.inference_mode():\n",
        "        logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
        "      if temperature > 0 :\n",
        "        # The temperature  is applied BEFORE the softmax\n",
        "        probs = self.model.forward(logits[:, -1] / temperature, dim = -1)\n",
        "        next_token = self._sample_top_p(probs, top_p)\n",
        "      else:\n",
        "        # Greedly select the token with the maximum probability\n",
        "        next_token = torch.argmax(logits[:, -1], dim = -1)\n",
        "      next_token = next_token.reshape(-1)\n",
        "      # only replace the token if it is a padding token\n",
        "      next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "      tokens[:, cur_pos] = next_token\n",
        "      # EOS is reached only if we found an EOS token for a padding position\n",
        "      eos_reached |= (~promt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id())\n",
        "      if all(eos_reached):\n",
        "        break\n",
        "    out_tokens=[]\n",
        "    out_text=[]\n",
        "    for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "      # Cut to the EOS token, if present\n",
        "      if self.tokenizer.eos_id() in current_prompt_tokens:\n",
        "        eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id())\n",
        "        current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "      out_tokens.append(current_prompt_tokens)\n",
        "      out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
        "    return (out_tokens, out_text)\n",
        "\n",
        "  def _sample_top_p(self, probs, p):\n",
        "    probs_sort, probs_idx = torch.sort(probs,dim = -1,descending = True)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    probs_sort[mask] = 0.0\n",
        "    probs_sort._div(probs_sort.sum(-1, keepdim=True))\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "\n",
        "    return next_token\n"
      ],
      "metadata": {
        "id": "HgBUJKJphoBl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "allow_cuda = False\n",
        "device = \"cuda\" if torch.cuda.is_available() and allow_cuda else \"cpu\"\n",
        "\n",
        "prompts = [\n",
        "    \" Simply put, the theory of relativity states that\",\n",
        "    \"If google was an italian company founded in Milan, it would\",\n",
        "    # Few shot prompt\n",
        "    \"\"\"Translate English to French:\n",
        "\n",
        "    sea otter => loutre de mer\n",
        "    peppermint -> menthe poivrée\n",
        "    plush giraffe => girafe peluche\n",
        "    cheese omelette =>\"\"\",\n",
        "    # Zero shot prompt\n",
        "    \"\"\" Tell me if the following person is actually Doraemon disguised as human:\n",
        "    Name: Umar Jamil\n",
        "    Decision:\"\"\"]\n",
        "\n",
        "model = LLaMA.build(\n",
        "    checkpoints_dir=\"llama-2-7b\",\n",
        "    tokenizer_path=\"./tokenizer.model\",\n",
        "    load_model=True,\n",
        "    max_seq_len=1024,\n",
        "    max_batch_size=len(prompts),\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhjvltaanBkD",
        "outputId": "663d5707-d649-4326-cbbe-67228a81e356"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint llama-2-7b/consolidated.00.pth\n",
            "Loaded checkpoint in 30.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded state dict in 79.11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference the model\n",
        "out_tokens, out_text = (model.text_completion(prompts, max_gen_len=64))\n",
        "assert len(out_text) == len(prompts)\n",
        "for i in range(len(out_text)):\n",
        "  print(f\"{out_text[i]}\")\n",
        "  print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "XMotm-8e112v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "6d2d467f-5fef-4c4a-aecd-8c940de9eaed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating...:   0%|          | 0/117 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-83f3468eb9e2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inference the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gen_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{out_text[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a743dad99a71>\u001b[0m in \u001b[0;36mtext_completion\u001b[0;34m(self, prompts, temperature, top_p, max_gen_len)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcur_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_pos\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcur_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# The temperature  is applied BEFORE the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-be6671eebf3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, start_pos)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# Consecutively apply all the encoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-be6671eebf3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, start_pos, freqs_complex)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_complex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# (B, seq_len, Dim) + (B, seq_len, Dim) -> (B, seq_len, Dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreqs_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-be6671eebf3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#(dim) * (B, Seq_len, dim) * (B, seq_len, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcxCPg55x6nJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}