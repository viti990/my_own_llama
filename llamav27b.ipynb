{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNUi+EFj7ghm+PREor5OYk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viti990/my_own_llama/blob/main/llamav27b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aviator LLaMA\n",
        "\n",
        "This notebook aims at reconstructing LLAMA 2 7B architecture and use the model weights trained from meta to learn how it is done!\n",
        "\n",
        "Also the aim is to adapt the model with LoRA and QLoRA for PEFT!\n",
        "\n",
        "The fine tuning will be done with aviation regulations from 14 CFR...\n"
      ],
      "metadata": {
        "id": "H57i8tK00XR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Downloading the weights\n",
        "\n",
        "Downloading the weights from meta for 7B-chat (i want the model to be able to respond to questions from the requirements)\n",
        "for this one must go to the meta website and request access, then go the github and use the download.sh scrip as shown below.\n"
      ],
      "metadata": {
        "id": "j5YxiWmS30_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/meta-llama/llama/\n",
        "!mv ./llama/download.sh ./\n",
        "!rm -rf llama\n",
        "!bash download.sh\n",
        "!rm -rf LICENSE USE_POLICY.md tokenizer_checklist.chk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTgN2c7aYSDA",
        "outputId": "55a16369-da2f-498d-b9a8-d16a07a2840f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 464 (delta 17), reused 33 (delta 12), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (464/464), 1.12 MiB | 21.96 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n",
            "Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "\n",
            "Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B-chat\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2024-07-04 17:38:41--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.113, 108.156.60.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7020 (6.9K) [binary/octet-stream]\n",
            "Saving to: ‘./LICENSE’\n",
            "\n",
            "./LICENSE           100%[===================>]   6.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-04 17:38:41 (393 MB/s) - ‘./LICENSE’ saved [7020/7020]\n",
            "\n",
            "--2024-07-04 17:38:41--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.113, 108.156.60.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4766 (4.7K) [binary/octet-stream]\n",
            "Saving to: ‘./USE_POLICY.md’\n",
            "\n",
            "./USE_POLICY.md     100%[===================>]   4.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-04 17:38:41 (442 MB/s) - ‘./USE_POLICY.md’ saved [4766/4766]\n",
            "\n",
            "Downloading tokenizer\n",
            "--2024-07-04 17:38:41--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.113, 108.156.60.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 499723 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer.model’\n",
            "\n",
            "./tokenizer.model   100%[===================>] 488.01K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-07-04 17:38:42 (25.7 MB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
            "\n",
            "--2024-07-04 17:38:42--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.113, 108.156.60.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50 [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer_checklist.chk’\n",
            "\n",
            "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-04 17:38:42 (6.87 MB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
            "\n",
            "tokenizer.model: OK\n",
            "Downloading llama-2-7b-chat\n",
            "--2024-07-04 17:38:42--  https://download.llamameta.net/llama-2-7b-chat/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.113, 108.156.60.62, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476925163 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/consolidated.00.pth’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]  12.55G   316MB/s    in 44s     \n",
            "\n",
            "2024-07-04 17:39:26 (295 MB/s) - ‘./llama-2-7b-chat/consolidated.00.pth’ saved [13476925163/13476925163]\n",
            "\n",
            "--2024-07-04 17:39:26--  https://download.llamameta.net/llama-2-7b-chat/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.56, 108.156.60.113, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [application/json]\n",
            "Saving to: ‘./llama-2-7b-chat/params.json’\n",
            "\n",
            "./llama-2-7b-chat/p 100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-04 17:39:26 (10.6 MB/s) - ‘./llama-2-7b-chat/params.json’ saved [102/102]\n",
            "\n",
            "--2024-07-04 17:39:26--  https://download.llamameta.net/llama-2-7b-chat/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoidGQxOHdlcHp2M21ubGl6YzJqZTQ0Nm40IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDE5ODM0OX19fV19&Signature=qbEw3yZ-aOH%7ElMJPCaqdLdxv6cFeKlza-RFTWxnd54SEePs6awC85oYIB1Qr9pk6dSp6kpBcBJRwtzJSEN0dj-WPmvUqVyZU4M3q2A-qakJUhO3hoo8lhroBxTYHOVCc0n8IQTMNLSkYsBzpu7Sfdeizkq%7EkBzCB140NNChA4QcjWgaF1P-KWPqOLAtgC1b1VPHo25qB1sbZTrOix3wmVatrHg79RjsjHbqIpL%7ECmZhBtpkauJG4JG2FMZNiPCH3NzG8am2rociEogyIFoeZTKBTKeE7NdOOHCNcFe6tanQMo7FmRXMWwnmeRW5ShwpU6fmfp1qaJH4iFYcQrVf-Kw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1616987309079799\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.56, 108.156.60.113, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100 [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/checklist.chk’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]     100  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-04 17:39:27 (12.1 MB/s) - ‘./llama-2-7b-chat/checklist.chk’ saved [100/100]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating the architecture"
      ],
      "metadata": {
        "id": "REMnXCWz4WaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Importing everything..."
      ],
      "metadata": {
        "id": "ClefDRdS42cY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zw7Pu7e8NUks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim: int=4096\n",
        "  n_layers: int=32\n",
        "  n_heads: int=32 #number of heads for the queries\n",
        "  n_kv_heads: Optional[int]=None #number of values for the key and value\n",
        "  vocab_size: int = -1 # This will be set when we load the tokenizer\n",
        "  multiple_of: int = 256\n",
        "  ffn_dim_multiplier: Optional[float]=None\n",
        "  norm_eps: float = 1e-5\n",
        "\n",
        "  #Needed for KV cache\n",
        "  max_batch_size: int=32\n",
        "  max_seq_len: int=2048\n",
        "\n",
        "  device: str=None\n",
        "\n",
        "def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0 ):\n",
        "  # as written in the paper, the dimension of the embedding must be even.\n",
        "  assert head_dim % 2 ==0, \"Dimension must be divisible by 2\"\n",
        "  # Build the theta parameters\n",
        "  # According to the formula theta_i = 10000^ (-2(i-1)/dim) for i = [1, 2, ... dim / 2]\n",
        "  # Shape: (head_dim / 2)\n",
        "  theta_numerator = torch.arange(0, head_dim, 2).float()\n",
        "  theta = 1.0 / (theta ** (theta_numerator/head_dim)).to(device)\n",
        "  # Construct the positions (the \"m\" parameter)\n",
        "  # shape: (seq_len)\n",
        "  m = torch.arange(seq_len, device=device)\n",
        "  # Multiply each theta by each position using the outer product\n",
        "  # Shape: {Seq_Len} outer_product * (head_dim / 2) -> (seq_len, head_dim / 2)\n",
        "  freqs = torch.outer(m, theta).float()\n",
        "  # we can compute complex numbers in the polar form c = R * exp(i * m * theta), where r = 1 as follows:\n",
        "  freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "  return freqs_complex\n",
        "\n",
        "def apply_rotary_embedding(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "  # (B, Seq_len, H, Head_dim) -> (B,Seq_len, H, head_dim/2)\n",
        "  x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:1], -1, 2))\n",
        "  # (Seq_len, head_dim / 2) -> (1, Seq_len, head_dim / 2)\n",
        "  freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  # (B, Seq_len, H, head_dim / 2) * (1, Seq_len, 1, head_dim / 2) * (B, seq_len, H, Head_Dim / 2)\n",
        "  x_rotated = x_complex * freqs_complex\n",
        "  # (B, seq_len, H, head_dim / 2) -> (B, seq_len, H, head_dim / 2, 2)\n",
        "  x_out = torch.view_as_real(x_rotated)\n",
        "  #(B, seq_len, H, head_dim / 2, 2) -> (B, seq_len, H, head_dim)\n",
        "  x_out = x_out.reshape(*x.shape)\n",
        "\n",
        "  return x_out.type_as(x).to(device)\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "  batch_size, seq_len, n_kv_heads = x.shape\n",
        "  if n_rep == 1:\n",
        "     return x\n",
        "  else:\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(batch_size,seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim: int, eps: float=1e-5) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # the gamma parameter\n",
        "    self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # (B, seq_len, dim)\n",
        "    # rsqrt: 1/sqrt(x)\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True)) + self.eps\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    #(dim) * (B, Seq_len, dim) * (B, seq_len, dim)\n",
        "    return self.weight * self._norm(x.float()).type_as(x)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    # Indicates the number of heads fot the key and values\n",
        "    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    # Indicates the number of heads for the queries\n",
        "    self.n_heads_q = args.n_heads\n",
        "    # Indicates how many times the heads of keys and values should be repeated to match the head of the queries\n",
        "    self.n_rep = self.n_heads_q // self.n_kv_heads\n",
        "    # Indicates the dimension of each head\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias = False)\n",
        "    self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias = False)\n",
        "    self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias = False)\n",
        "    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim,  bias = False)\n",
        "\n",
        "    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    batch_size, seq_len, _ = x.shape #(B, 1, dim)\n",
        "\n",
        "    # Apply the wq, wk, wv matrices to queries, keys and values\n",
        "    # (B, 1, dim) -> (B, 1, H_Q * head_dim)\n",
        "    xq = self.wq(x)\n",
        "\n",
        "    # (B, 1, dim) -> (B, 1, H_KV * head_dim)\n",
        "    xk = self.wk(x)\n",
        "    xv = self.wv(x)\n",
        "\n",
        "    # (B, 1, H_Q * head_dim) -> (B, 1, H_Q, head_dim)\n",
        "    xq = xq.view(batch_size, 1, self.n_heads_q, self.head_dim)\n",
        "\n",
        "    # (B, 1, H_KV * head_dim) -> (B, 1, H_KV, head_dim)\n",
        "    xk = xk.view(batch_size, 1, self.n_kv_heads, self.head_dim)\n",
        "    xv = xv.view(batch_size, 1, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "    # Does not change the shape of the tensors\n",
        "    xq = apply_rotary_embedding(xq, freqs_complex, device = x.device)\n",
        "    xk = apply_rotary_embedding(xk, freqs_complex, device = x.device)\n",
        "\n",
        "    # Replace the entry in the cache fot this token\n",
        "    self.cache_k[:batch_size, start_pos:start_pos+seq_len] = xk\n",
        "    self.cache_v[:batch_size, start_pos:start_pos+seq_len] = xv\n",
        "\n",
        "    # Retrieve all the cached keys and values so far\n",
        "    # (B, seq_len_KV, H_KV, head_dim)\n",
        "    keys = self.cache_k[:batch_size, 0:start_pos+seq_len]\n",
        "    values = self.cache_v[:batch_size, 0:start_pos+seq_len]\n",
        "\n",
        "    # Repeat the heads of the K and V to reach the number of heads of the queries\n",
        "    keys = repeat_kv(keys, self.n_rep)\n",
        "    values = repeat_kv(values, self.n_rep)\n",
        "\n",
        "    # (B, 1, H_Q, head_dim) -> (B, H_Q, 1, head_dim)\n",
        "    xq = xq.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # (B, H_Q, 1, seq_len) @ (B, H_Q, seq_len_kv, head_dim) -> (B, H_Q, 1, head_dim)\n",
        "    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "    scores = F.softmax(scores, dim=-1).type_as(xq)\n",
        "\n",
        "    # (B, H_Q, 1, head_dim) -> (B, 1, H_Q, head_dim) -> (B, 1, dim)\n",
        "    output = torch.matmul(scores, values)\n",
        "    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "\n",
        "    return self.wo(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    hidden_dim = 4 * args.dim\n",
        "    hidden_dim = int(2 * hidden_dim / 3)\n",
        "    if args.ffn_dim_multiplier is not None:\n",
        "      hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
        "    # round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
        "    hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "\n",
        "    self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "    self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
        "    self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    swish = F.silu(self.w1(x))\n",
        "    x_V = self.w3(x)\n",
        "    x = swish * x_V\n",
        "    x= self.w2(x)\n",
        "    return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_head=args.n_heads\n",
        "    self.dim = args.dim\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "    self.attention = SelfAttention(args)\n",
        "    self.feed_forward = FeedForward(args)\n",
        "\n",
        "    # Normalization BEFORE the self attention\n",
        "    self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    # Normalization BEFORE the feed forward block\n",
        "    self.ffn_norm = RMSNorm(args.dim, eps= args.norm_eps)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
        "    # (B, seq_len, Dim) + (B, seq_len, Dim) -> (B, seq_len, Dim)\n",
        "    h = x + self.attention.forward(self.attention_norm(x), start_pos,freqs_complex)\n",
        "    out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, args: ModelArgs) -> None:\n",
        "    super().__init__()\n",
        "    assert args.vocab_size != -1, \"Vocab size must be set\"\n",
        "\n",
        "    self.args = args\n",
        "    self.vocab_size = args.vocab_size\n",
        "    self.n_layers = args.n_layers\n",
        "    self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "    for _ in range(args.n_layers):\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "\n",
        "    self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_length * 2, device=self.args.device)\n",
        "\n",
        "  def forward(self, tokens: torch.Tensor, start_pos: int):\n",
        "    # (B, seq_len)\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"only one token at a time can be processed\" #this only works for inference for training you must process more than 1 token at a time and must also remove KV cache\n",
        "\n",
        "    # (B, Seq_Len) -> (B, seq_len, Dim)\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos+seq_len]\n",
        "\n",
        "    # Consecutively apply all the encoder layers\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "\n",
        "    output = self.output(h).float()\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "__qz9L6dqfKN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from model import ModelArgs, Transformer\n",
        "\n",
        "class LLaMA:\n",
        "\n",
        "  def __init__(self, model: Transformer, tokenizer: SentencePieceProcessor, model_args: ModelArgs):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.args = model_args\n",
        "\n",
        "  @staticmethod\n",
        "  def build(checkpoints_dir: str, tokenizer_path: str, load_model: bool, max_seq_len: int, max_batch_size: int, device: str):\n",
        "    prev_time = time.time()\n",
        "    if load_model:\n",
        "      checkpoints = sorted(Path(checkpoints_dir).glob('*.pth'))\n",
        "      assert len(checkpoints) > 0, \"No checkpoints files found\"\n",
        "      chk_path = checkpoints[0]\n",
        "      print(f\"Loading checkpoint {chk_path}\")\n",
        "      checkpoint = torch.load(chk_path, map_location=\"cpu\")\n",
        "      print(f\"Loaded checkpoint in {(time.time() - prev_time):.2f}s\")\n",
        "      prev_time = time.time()\n",
        "\n",
        "    with open (Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
        "      params = json.loads(f.read())\n",
        "\n",
        "    model_args: ModelArgs = ModelArgs(\n",
        "      max_seq_len=max_seq_len,\n",
        "      max_batch_size=max_batch_size,\n",
        "      device=device,\n",
        "      **params\n",
        "    )\n",
        "\n",
        "    tokenizer = SentencePieceProcessor()\n",
        "    tokenizer.load(tokenizer_path)\n",
        "    model_args.vocab_size = tokenizer.vocab_size()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "      torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "    else:\n",
        "      torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
        "\n",
        "    model = Transformer(model_args).to(device)\n",
        "    if load_model:\n",
        "      del checkpoint[\"rope.freqs\"]\n",
        "      model.load_state_dict(checkpoint, strict=True)\n",
        "      print(f\"loaded state dict in {(time.time() - prev_time):.2f}s\")\n",
        "\n",
        "    return LLaMA(model, tokenizer, model_args)"
      ],
      "metadata": {
        "id": "HgBUJKJphoBl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "allow_cuda = False\n",
        "device = \"cuda\" if torch.cuda.is_available() and allow_cuda else \"cpu\"\n",
        "\n",
        "model = LLaMA.build(\n",
        "    checkpoints_dir=\"llama-2-7b-chat\",\n",
        "    tokenizer_path=\"./tokenizer.model\",\n",
        "    load_model=True,\n",
        "    max_seq_len=1024,\n",
        "    max_batch_size=3,\n",
        "    device=device\n",
        ")\n",
        "print(\"All ok\")\n"
      ],
      "metadata": {
        "id": "JhjvltaanBkD",
        "outputId": "0f9639a5-f127-48ff-a0ca-ca328401c09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint llama-2-7b-chat/consolidated.00.pth\n",
            "Loaded checkpoint in 5.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:431.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2a2938fa24f4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model = LLaMA.build(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcheckpoints_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-2-7b-chat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokenizer_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./tokenizer.model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-56e89024b86d>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(checkpoints_dir, tokenizer_path, load_model, max_seq_len, max_batch_size, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBFloat16Tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rope.freqs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e9b81164b444>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e9b81164b444>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e9b81164b444>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_kv_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_kv_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del model\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mzlQvNfQxQ-4",
        "outputId": "5f1213ef-257f-4c9d-8860-37028f33a110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-68e6f215253e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMotm-8e112v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}